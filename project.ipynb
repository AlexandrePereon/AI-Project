{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire de Données (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ydata_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "extract_path = './AI_Project_Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données utilisées dans ce projet proviennent de plusieurs sources et sont fournies sous forme de fichiers CSV. Chaque fichier contient des informations spécifiques concernant les employés, leur historique professionnel et leur environnement de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_employee_survey_data = os.path.join(extract_path, 'employee_survey_data.csv')\n",
    "csv_manager_survey_data = os.path.join(extract_path, 'manager_survey_data.csv')\n",
    "csv_general_data = os.path.join(extract_path, 'general_data.csv')\n",
    "csv_out_time = os.path.join(extract_path, 'out_time.csv')\n",
    "csv_in_time = os.path.join(extract_path, 'in_time.csv')\n",
    "\n",
    "employee_survey_df = pd.read_csv(csv_employee_survey_data)\n",
    "manager_survey_df = pd.read_csv(csv_manager_survey_data)\n",
    "general_df = pd.read_csv(csv_general_data)\n",
    "out_time_df = pd.read_csv(csv_out_time)\n",
    "in_time_df = pd.read_csv(csv_in_time)\n",
    "\n",
    "# init empty dataframe\n",
    "work_info = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs dupliquées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aucune ligne en double n'a été trouvée dans les différentes données. Néanmoins, une vérification a été réalisée pour s'assurer qu'aucun employé n'apparaît plusieurs fois avec des informations redondantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_info.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes constantes ont été supprimées, car elles n’apportent aucune variation dans les données et ne contribuent donc pas à l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_columns(df):\n",
    "    return df.loc[:, df.nunique() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur la plus fréquente a été utilisée pour imputer les valeurs manquantes dans les variables catégorielles, car elle représente la modalité dominante. Cela permet de minimiser la perturbation des données et de conserver la cohérence des catégories existantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical_na(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df.loc[:, categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La médiane a été utilisée pour imputer les valeurs absentes, car elle est moins sensible aux valeurs extrêmes que la moyenne. Cela permet de préserver la distribution des données et d'éviter les biais induits par d'éventuelles valeurs aberrantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric_na(df):\n",
    "    for col in df.select_dtypes(include=['number', 'float64']).columns:\n",
    "        median_value = df[col].median()\n",
    "        df.loc[:, col] = df[col].fillna(round(median_value))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'une valeur était manquante pour TotalWorkingYears, elle a été remplacée par la valeur de YearsAtCompany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_total_working_years(df):\n",
    "    if 'TotalWorkingYears' in df.columns and 'YearsAtCompany' in df.columns:\n",
    "        df.loc[:, 'TotalWorkingYears'] = df['TotalWorkingYears'].fillna(df['YearsAtCompany'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type des valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification des types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes numériques ont été converties en int lorsque cela était possible, en particulier si elles ne contenaient que des valeurs entières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_numeric_columns(df):\n",
    "    df = df.apply(lambda col: col.astype(int) if col.dtype == 'float64' and col.dropna().mod(1).eq(0).all() else col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des types `object` en booléen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_attrition_to_bool(df):\n",
    "    if 'Attrition' in df.columns:\n",
    "        df.loc[:, 'Attrition'] = df['Attrition'].map({'Yes': True, 'No': False})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des fichiers contenant les horaires d'entrée et sortie des salariés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fichiers in_time et out_time ont été restructurés pour créer une table unifiée indiquant les heures d'arrivée et de départ des employés par jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_and_merge(df):\n",
    "    in_time_melted = in_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='arrival_time')\n",
    "    out_time_melted = out_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='departure_time')\n",
    "\n",
    "    # Renommer la colonne EmployeeID\n",
    "    in_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "    out_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur 'id' et 'date'\n",
    "    merged_clock_in = pd.merge(in_time_melted, out_time_melted, on=['EmployeeID', 'date'], how='outer')\n",
    "    return merged_clock_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_work_column(df):\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'])\n",
    "\n",
    "    # Calculer le temps travaillé (différence entre départ et arrivée)\n",
    "    df['worked_time'] = df['departure_time'] - df['arrival_time']\n",
    "\n",
    "    # Convertir en heures pour avoir un format lisible\n",
    "    df['worked_hours'] = df['worked_time'].dt.total_seconds() / 3600\n",
    "\n",
    "    # Trier par id et date\n",
    "    df.sort_values(by=['EmployeeID', 'date'], inplace=True)\n",
    "\n",
    "    # Moyenne de la durée de travail par jour pour chaque employé\n",
    "    mean_worked_hours = df.groupby('EmployeeID')['worked_hours'].mean()\n",
    "\n",
    "    # Nombre total d'heures travaillées par employé\n",
    "    total_worked_hours = df.groupby('EmployeeID')['worked_hours'].sum()\n",
    "\n",
    "    # Nombre de jours ou le worked_hours est non nul\n",
    "    worked_days = df[df['worked_hours'] > 0].groupby('EmployeeID')['worked_hours'].count()\n",
    "\n",
    "    # Faire un data frame avec ces 3 informations pour chaque employé avec l'EmployeeID comme index\n",
    "    work_info = pd.concat([mean_worked_hours, total_worked_hours, worked_days], axis=1)\n",
    "    work_info.columns = ['mean_worked_hours', 'total_worked_hours', 'worked_days']\n",
    "    return work_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertion des types numériques en types `object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes numériques présentant des caractéristiques catégoriques ont été converties afin d’être cohérentes avec le reste des colonnes catégoriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employee_survey_to_cat(df):\n",
    "    df['EnvironmentSatisfaction'] = df['EnvironmentSatisfaction'].replace({1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Excellent'})\n",
    "    df['JobSatisfaction'] = df['JobSatisfaction'].replace({1: 'Dissatisfied', 2: 'Neutral', 3: 'Satisfied', 4: 'Very Satisfied'})\n",
    "    df['WorkLifeBalance'] = df['WorkLifeBalance'].replace({1: 'Poor', 2: 'Average', 3: 'Good', 4: 'Excellent'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manager_survey_to_cat(df):\n",
    "    df['JobInvolvement'] = df['JobInvolvement'].replace({1: 'Not Engaged', 2: 'Moderately Engaged', 3: 'Highly Engaged', 4: 'Fully Committed'})\n",
    "    df['PerformanceRating'] = df['PerformanceRating'].replace({1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Outstanding'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_to_cat(df):\n",
    "    df['Education'] = df['Education'].replace({\n",
    "        1: 'HighSchool', \n",
    "        2: 'Associate', \n",
    "        3: 'Bachelor', \n",
    "        4: 'Master', \n",
    "        5: 'PhD'\n",
    "    })\n",
    "    df['StockOptionLevel'] = df['StockOptionLevel'].replace({  \n",
    "        0: 'None',  \n",
    "        1: 'Low',  \n",
    "        2: 'Medium',  \n",
    "        3: 'High'  \n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prétraitement de chacun de nos jeux de données de façon indépendante, via des pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_preprocessor = Pipeline([\n",
    "    ('remove_constants', FunctionTransformer(remove_constant_columns, validate=False)),\n",
    "    ('fill_categorical_na', FunctionTransformer(fill_categorical_na, validate=False)),\n",
    "    ('fill_numeric_na', FunctionTransformer(fill_numeric_na, validate=False)),\n",
    "    ('simplify_numeric_columns', FunctionTransformer(simplify_numeric_columns, validate=False)),\n",
    "])\n",
    "\n",
    "employee_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "    ('employee_survey_to_cat', FunctionTransformer(employee_survey_to_cat, validate=False)),\n",
    "])\n",
    "\n",
    "manager_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "    ('manager_survey_to_cat', FunctionTransformer(manager_survey_to_cat, validate=False)),\n",
    "])\n",
    "\n",
    "general_preprocessor = Pipeline([\n",
    "    ('fill_total_working_years', FunctionTransformer(fill_total_working_years, validate=False)),\n",
    "    ('transform_attrition_to_bool', FunctionTransformer(transform_attrition_to_bool, validate=False)),\n",
    "    ('general_to_cat', FunctionTransformer(general_to_cat, validate=False)),\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "])\n",
    "\n",
    "work_info_preprocessor = Pipeline([\n",
    "    ('reverse_and_merge', FunctionTransformer(reverse_and_merge, validate=False)),\n",
    "    ('generate_work_column', FunctionTransformer(generate_work_column, validate=False)),\n",
    "])\n",
    "\n",
    "# Nettoyage des données\n",
    "employee_survey_data = employee_survey_preprocessor.fit_transform(employee_survey_df)\n",
    "manager_survey_data = manager_survey_preprocessor.fit_transform(manager_survey_df)\n",
    "general_data = general_preprocessor.fit_transform(general_df)\n",
    "work_info = work_info_preprocessor.fit_transform(work_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = general_data.merge(employee_survey_data, on='EmployeeID').merge(manager_survey_data, on='EmployeeID').merge(work_info, on='EmployeeID')\n",
    "\n",
    "clean_data = clean_data.drop(columns=['EmployeeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_num = clean_data.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des relations à travers un pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(clean_data[clean_data_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des relations à travers une heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(clean_data[clean_data_num].corr(), cmap='coolwarm')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables catégorielles ont été transformées en variables numériques à l'aide de l'encodage one-hot. Cette technique consiste à créer des colonnes binaires pour chaque catégorie, indiquant la présence (1) ou l'absence (0) de cette catégorie pour chaque observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = clean_data.select_dtypes(include=[object]).drop(columns=['Attrition'])\n",
    "\n",
    "clean_data_cat = pd.get_dummies(cat_columns, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "clean_data_num = pd.DataFrame(scaler.fit_transform(clean_data[clean_data_num]), columns=clean_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = pd.concat([clean_data_num, clean_data_cat, clean_data['Attrition']], axis=1)\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = concat_data[concat_data['Attrition'] == True]\n",
    "df_false = concat_data[concat_data['Attrition'] == False]\n",
    "\n",
    "\n",
    "min_count = min(len(df_true), len(df_false))\n",
    "\n",
    "df_true_sampled = df_true.sample(n=min_count, random_state=42)\n",
    "df_false_sampled = df_false.sample(n=min_count, random_state=42)\n",
    "\n",
    "\n",
    "full_data = pd.concat([df_true_sampled, df_false_sampled])\n",
    "full_data['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YDataProfiling : résumé des différents graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ydata-profiling` est une bibliothèque utilisée pour l'exploration automatique des données et la génération de rapports détaillés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_report = ydata_profiling.ProfileReport(full_data, title='Full Data')\n",
    "#final_data_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportation du dataset modifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(os.path.join(extract_path, 'cleaned_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des caractéristiques ayant le plus d'impact sur le taux d'attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour identifier les facteurs impactant le taux d'attrition, nous avons utilisé plusieurs méthodes :\n",
    "\n",
    "- VarianceThreshold pour éliminer les variables quasi-constantes.\n",
    "- SelectFromModel avec RandomForest pour sélectionner les variables importantes basées sur leur score d'importance.\n",
    "- RFE pour éliminer itérativement les variables moins pertinentes. Finalement, nous avons utilisé les coefficients du meilleur modèle pour identifier les variables les plus influentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_feature_selection_methods(X, y, k_features):\n",
    "    # Dictionnaire pour stocker les features sélectionnées par chaque méthode\n",
    "    selected_features = defaultdict(list)\n",
    "    \n",
    "    # VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.5)\n",
    "    selector.fit_transform(X)\n",
    "    selected_features['VarianceThreshold'] = list(X.columns[selector.get_support()])\n",
    "\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['SelectFromModel'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # RFE\n",
    "    selector = RFE(RandomForestClassifier(n_estimators=100), \n",
    "                  n_features_to_select=k_features, \n",
    "                  step=1)\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['RFE'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # Créer un DataFrame avec le nombre maximal de caractéristiques\n",
    "    max_features = max(len(features) for features in selected_features.values())\n",
    "    \n",
    "    # Remplir avec None pour avoir des colonnes de même longueur\n",
    "    for method in selected_features:\n",
    "        selected_features[method].extend([None] * (max_features - len(selected_features[method])))\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    features_table = pd.DataFrame(selected_features)\n",
    "    \n",
    "    # Ajouter des statistiques sur les features sélectionnées\n",
    "    print(\"\\nNombre de caractéristiques sélectionnées par méthode:\")\n",
    "    for method in selected_features:\n",
    "        count = sum(1 for x in selected_features[method] if x is not None)\n",
    "        print(f\"{method}: {count} features\")\n",
    "    \n",
    "    return features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation\n",
    "X = full_data.drop(columns=['Attrition'])\n",
    "y = full_data['Attrition'].astype(int)\n",
    "\n",
    "features_table = compare_feature_selection_methods(X, y, k_features=15)\n",
    "print(\"\\nTableau des caractéristiques sélectionnées:\")\n",
    "features_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des critères majeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette analyse permettra de déterminer les actions à mettre en place par l'entreprise afin d'améliorer son taux d'attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse pour l'Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(min(clean_data[\"Age\"]), \n",
    "                 max(clean_data[\"Age\"]) + 3, \n",
    "                 3)\n",
    "\n",
    "# Ajout d'une nouvelle colonne avec les distances regroupées\n",
    "clean_data[\"AgeGrouped\"] = pd.cut(clean_data[\"Age\"], bins=bins, right=False)\n",
    "\n",
    "# Regroupement des données\n",
    "df_binned = clean_data.groupby(\"AgeGrouped\")[\"Attrition\"].agg([\"count\", \"mean\"]).reset_index()\n",
    "df_binned.columns = [\"AgeGrouped\", \"NombreEmployes\", \"TauxAttrition\"]\n",
    "df_binned[\"TauxAttrition\"] = df_binned[\"TauxAttrition\"] * 100\n",
    "\n",
    "# Configuration du style du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bars = plt.bar(df_binned[\"AgeGrouped\"].astype(str), df_binned[\"TauxAttrition\"], \n",
    "               color='green', alpha=0.7)\n",
    "\n",
    "plt.title(\"Taux d'Attrition selon l'âge\", pad=20, fontsize=14)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Taux d'Attrition (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = round(bar.get_height(), 1)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height}%',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse pour la distance de la maison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(min(clean_data[\"DistanceFromHome\"]), \n",
    "                 max(clean_data[\"DistanceFromHome\"]) + 3, \n",
    "                 3)\n",
    "\n",
    "# Ajout d'une nouvelle colonne avec les distances regroupées\n",
    "clean_data[\"DistanceGrouped\"] = pd.cut(clean_data[\"DistanceFromHome\"], bins=bins, right=False)\n",
    "\n",
    "# Regroupement des données\n",
    "df_binned = clean_data.groupby(\"DistanceGrouped\")[\"Attrition\"].agg([\"count\", \"mean\"]).reset_index()\n",
    "df_binned.columns = [\"DistanceGrouped\", \"NombreEmployes\", \"TauxAttrition\"]\n",
    "df_binned[\"TauxAttrition\"] = df_binned[\"TauxAttrition\"] * 100\n",
    "\n",
    "# Configuration du style du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bars = plt.bar(df_binned[\"DistanceGrouped\"].astype(str), df_binned[\"TauxAttrition\"], \n",
    "               color='green', alpha=0.7)\n",
    "\n",
    "plt.title(\"Taux d'Attrition selon la distance de la maison\", pad=20, fontsize=14)\n",
    "plt.xlabel(\"Distance de la maison\")\n",
    "plt.ylabel(\"Taux d'Attrition (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = round(bar.get_height(), 1)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height}%',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse selon le niveau hierarchique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(min(clean_data[\"JobLevel\"]), \n",
    "                 max(clean_data[\"JobLevel\"]) + 2, \n",
    "                 1)\n",
    "\n",
    "# Création d'un DataFrame avec les colonnes séparées\n",
    "df_binned = clean_data.groupby(\"JobLevel\")[\"Attrition\"].agg([\"count\", \"mean\"]).reset_index()\n",
    "df_binned.columns = [\"JobLevel\", \"NombreEmployes\", \"TauxAttrition\"]\n",
    "df_binned[\"TauxAttrition\"] = df_binned[\"TauxAttrition\"] * 100\n",
    "\n",
    "# Configuration du style du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bars = plt.bar(df_binned[\"JobLevel\"], df_binned[\"TauxAttrition\"], \n",
    "               color='green', alpha=0.7)\n",
    "plt.title(\"Taux d'Attrition selon le niveau hiérarchique\", pad=20, fontsize=14)\n",
    "plt.xlabel(\"Niveau Hiérarchique\")\n",
    "plt.ylabel(\"Taux d'Attrition (%)\")\n",
    "plt.xticks(df_binned[\"JobLevel\"].astype(int))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = round(bar.get_height(), 1)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height}%',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse pour le nombre d'entreprises précédentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(min(clean_data[\"NumCompaniesWorked\"]), \n",
    "                 max(clean_data[\"NumCompaniesWorked\"]) + 2, \n",
    "                 1)\n",
    "\n",
    "# Création d'un DataFrame avec les colonnes séparées\n",
    "df_binned = clean_data.groupby(\"NumCompaniesWorked\")[\"Attrition\"].agg([\"count\", \"mean\"]).reset_index()\n",
    "df_binned.columns = [\"NumCompaniesWorked\", \"NombreEmployes\", \"TauxAttrition\"]\n",
    "df_binned[\"TauxAttrition\"] = df_binned[\"TauxAttrition\"] * 100\n",
    "\n",
    "# Configuration du style du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bars = plt.bar(df_binned[\"NumCompaniesWorked\"], df_binned[\"TauxAttrition\"], \n",
    "               color='green', alpha=0.7)\n",
    "plt.title(\"Taux d'Attrition par Nombre d'Entreprises Précédentes\", pad=20, fontsize=14)\n",
    "plt.xlabel(\"Nombre d'Entreprises Précédentes\")\n",
    "plt.ylabel(\"Taux d'Attrition (%)\")\n",
    "plt.xticks(df_binned[\"NumCompaniesWorked\"].astype(int))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = round(bar.get_height(), 1)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height}%',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse pour la moyenne d'heures travaillées par jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(min(clean_data[\"mean_worked_hours\"]), \n",
    "                max(clean_data[\"mean_worked_hours\"]) + 1, \n",
    "                1)\n",
    "\n",
    "# Configuration du style\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Taux d'attrition par tranche horaire\n",
    "df_binned = pd.DataFrame({\n",
    "    'HeuresBin': pd.cut(clean_data[\"mean_worked_hours\"], bins=bins),\n",
    "    'Attrition': clean_data[\"Attrition\"]\n",
    "})\n",
    "\n",
    "attrition_rate = df_binned.groupby('HeuresBin')['Attrition'].agg(['count', 'mean']).reset_index()\n",
    "attrition_rate.columns = ['HeuresBin', 'NombreEmployes', 'TauxAttrition']\n",
    "attrition_rate['TauxAttrition'] = attrition_rate['TauxAttrition'] * 100\n",
    "\n",
    "bars = plt.bar(range(len(attrition_rate)), attrition_rate['TauxAttrition'], \n",
    "            color='green', alpha=0.7)\n",
    "plt.title('Taux d\\'Attrition par Tranche Horaire', pad=20, fontsize=14)\n",
    "plt.xlabel('Heures Travaillées en Moyenne par Jour')\n",
    "plt.ylabel('Taux d\\'Attrition (%)')\n",
    "plt.xticks(range(len(attrition_rate)), \n",
    "          [f\"{interval.left:.1f}-{interval.right:.1f}h\" \n",
    "           for interval in attrition_rate['HeuresBin']], \n",
    "          rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = round(bar.get_height(), 1)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélectionner le modèle le plus performant pour prédire l'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Load the data\n",
    "filename = './AI_Project_Data/cleaned_data.csv'\n",
    "employee_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des valeurs d'attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation du dataset en plusieurs échantillons pour la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "employee_df = employee_data.copy()\n",
    "\n",
    "y = employee_df[\"Attrition\"]\n",
    "X = employee_df.drop(columns=[\"Attrition\"])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42), \n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des dictionnaires pour stocker les scores\n",
    "scores = []\n",
    "predictions = {}\n",
    "metrics = {\n",
    "    'precision': {name: [] for name in models.keys()},\n",
    "    'recall': {name: [] for name in models.keys()},\n",
    "    'f1': {name: [] for name in models.keys()},\n",
    "    'auc': {name: [] for name in models.keys()},\n",
    "    'accuracy': {name: [] for name in models.keys()}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comparer nos résultats sur les différents modèles, nous avons tracé la courbe ROC correspondant à chacun des 4 modèles choisis afin d’évaluer leur capacité à distinguer les classes positives et négatives. La courbe ROC représente le taux de vrais positifs en fonction du taux de faux positifs pour plusieurs seuils de classification, permettant ainsi d’analyser et de comparer leurs performances.\n",
    "\n",
    "Nous obtenons des résultats différents pour chaque itération de la validation croisée (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Prédictions\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions[name] = model.predict(X_test)    \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        # Calcul des métriques\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba) if y_proba is not None else (None, None, None)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.2f})\")\n",
    "        \n",
    "        # Stockage des métriques\n",
    "        metrics['precision'][name].append(precision)\n",
    "        metrics['recall'][name].append(recall)\n",
    "        metrics['f1'][name].append(f1)\n",
    "        metrics['auc'][name].append(auc)\n",
    "        metrics['accuracy'][name].append(accuracy)\n",
    "        # Ajout des scores dans le DataFrame final\n",
    "        scores.append({\n",
    "            'Model': name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Accuracy': accuracy,\n",
    "            'AUC': auc\n",
    "        })\n",
    "    plt.xlabel(\"Taux de Faux Positifs\")\n",
    "    plt.ylabel(\"Taux de Vrais Positifs\")\n",
    "    plt.title(\"Courbe ROC\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analyse des scores des modèles selon les différentes métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df_mean = scores_df.groupby('Model').mean()\n",
    "scores_df_std = scores_df.groupby('Model').std()\n",
    "scores_df_mean = scores_df_mean.sort_values(['F1 Score', 'Precision', 'Recall', 'Accuracy'], ascending=False) \n",
    "scores_df_std = scores_df_std.sort_values(['F1 Score', 'Precision', 'Recall', 'Accuracy'], ascending=True) \n",
    "\n",
    "print('Mean Scores')\n",
    "print(scores_df_mean)\n",
    "print('_'*50)\n",
    "print('Standard Deviation')\n",
    "print(scores_df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des temps d'entrainement et de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "training_times = []\n",
    "prediction_times = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    training_times.append(end_time - start_time)\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    prediction_times.append(end_time - start_time)\n",
    "\n",
    "# Create DataFrame to store times\n",
    "time_df = pd.DataFrame({\n",
    "    'Model': models.keys(),\n",
    "    'Training Time (s)': training_times,\n",
    "    'Prediction Time (s)': prediction_times\n",
    "})\n",
    "\n",
    "time_df = time_df.sort_values(['Prediction Time (s)', 'Training Time (s)'], ascending=True) \n",
    "print(time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'importance des différentes variables du dataset sur la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def get_feature_importance(model, X, y, model_name):\n",
    "    feature_names = X.columns\n",
    "    importance_dict = {}\n",
    "    \n",
    "    if model_name == \"RandomForest\" or model_name == \"DecisionTreeClassifier\":\n",
    "        # Ces modèles ont feature_importances_ natif\n",
    "        importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "        \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        # Pour la régression logistique, on utilise les coefficients\n",
    "        if len(model.classes_) == 2:  # Classification binaire\n",
    "            importance_dict = dict(zip(feature_names, np.abs(model.coef_[0])))\n",
    "        else:  # Classification multi-classe\n",
    "            # Moyenne des valeurs absolues des coefficients pour chaque classe\n",
    "            importance_dict = dict(zip(feature_names, np.mean(np.abs(model.coef_), axis=0)))\n",
    "            \n",
    "    elif model_name == \"SVC\":\n",
    "        # Pour SVC, on utilise permutation_importance car pas d'importance native\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "        importance_dict = dict(zip(feature_names, result.importances_mean))\n",
    "    \n",
    "    return importance_dict\n",
    "\n",
    "def plot_feature_importance(importance_dict, model_name, top_n=50):\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Conversion en DataFrame pour faciliter la manipulation\n",
    "    df_importance = pd.DataFrame({\n",
    "        'feature': importance_dict.keys(),\n",
    "        'importance': importance_dict.values()\n",
    "    })\n",
    "    \n",
    "    # Tri par importance décroissante et sélection des top_n features\n",
    "    df_importance = df_importance.sort_values('importance', ascending=True).tail(top_n)\n",
    "    \n",
    "    # Création du graphique\n",
    "    sns.barplot(data=df_importance, x='importance', y='feature', palette=\"viridis\")\n",
    "    \n",
    "    plt.title(f'Top {top_n} Features Importance - {model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for i, v in enumerate(df_importance['importance']):\n",
    "        plt.text(v, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name} Feature Importance:\")\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calcul et affichage des importances\n",
    "    importance_dict = get_feature_importance(model, X, y, model_name)\n",
    "    \n",
    "    # Visualisation\n",
    "    plot_feature_importance(importance_dict, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser le meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La validation croisée à 5 plis a été employée pour évaluer la robustesse des modèles tout en réduisant le risque de sur-ajustement. Plusieurs métriques ont été prises en compte, notamment l’accuracy, le score F1, la précision, le rappel et l’aire sous la courbe ROC (AUC-ROC), afin d’obtenir une évaluation complète des performances. Le critère de sélection du meilleur estimateur pour la validation croisée a été basé sur l’accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_params_grid = {\n",
    "    # Paramètres les plus importants\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    \n",
    "    # Paramètre de base\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'random_state': [42],\n",
    "    'probability': [True]\n",
    "}\n",
    "\n",
    "# Creation du model avec GridSearchCV\n",
    "SVC_model = GridSearchCV(\n",
    "    estimator=SVC(probability=True),\n",
    "    param_grid=svc_params_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',\n",
    "    verbose=0,\n",
    "    error_score='raise',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Définition d'une grille simplifiée de paramètres\n",
    "rf_params_grid = {\n",
    "    # Paramètres essentiels\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    \n",
    "    # Paramètres de base\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Configuration du GridSearchCV avec des options avancées\n",
    "RandomForest_model = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=rf_params_grid,\n",
    "    cv=5,  # Validation croisée à 5 plis\n",
    "    n_jobs=-1,  # Utilise tous les cœurs disponibles\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',  # Réentraîne sur la meilleure métrique accuracy\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Apprentissage du model\n",
    "RandomForest_model.fit(X_train, y_train)\n",
    "SVC_model.fit(X_train, y_train)\n",
    "\n",
    "# Récupération de meilleurs paramètres\n",
    "best_rf_model = RandomForest_model.best_estimator_\n",
    "best_svc_model = SVC_model.best_estimator_\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nMeilleurs paramètres trouvés :\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_params_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_params_}\")\n",
    "print(\"\\nMeilleur score de validation croisée:\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_score_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_score_}\")\n",
    "\n",
    "\n",
    "# Affichage des scores par métrique \n",
    "rf_results = pd.DataFrame(RandomForest_model.cv_results_)\n",
    "svc_results = pd.DataFrame(SVC_model.cv_results_)\n",
    "print(\"\\nRésultats détaillés pour la meilleure configuration:\")\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de la courbe ROC après optimisaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf_model.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, label=f\"RandomForest (AUC = {auc:.2f})\")\n",
    "\n",
    "\n",
    "y_pred = best_svc_model.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "plt.plot(fpr, tpr, label=f\"SVC (AUC = {auc:.2f})\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Taux de Faux Positifs\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs\")\n",
    "plt.title(\"Courbe ROC RandomForest vs SVC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des principaux résultats de la RandomForest pour chaque pli de la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des modèles après optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour la visualisation\n",
    "def prepare_comparison_data(rf_results, svc_results, metrics):\n",
    "    # Création d'un dataframe pour la comparaison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Random Forest\n",
    "        rf_scores = {\n",
    "            'model': 'Random Forest',\n",
    "            'metric': metric,\n",
    "            'score': rf_results[f'mean_test_{metric}'].mean(),\n",
    "            'std': rf_results[f'std_test_{metric}'].mean()\n",
    "        }\n",
    "        comparison_data.append(rf_scores)\n",
    "        \n",
    "        # SVC\n",
    "        svc_scores = {\n",
    "            'model': 'SVC',\n",
    "            'metric': metric,\n",
    "            'score': svc_results[f'mean_test_{metric}'].mean(),\n",
    "            'std': svc_results[f'std_test_{metric}'].mean()\n",
    "        }\n",
    "        comparison_data.append(svc_scores)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# 1. Graphique comparatif des métriques\n",
    "def plot_metrics_comparison(comparison_data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Création du barplot\n",
    "    ax = sns.barplot(\n",
    "        data=comparison_data,\n",
    "        x='metric',\n",
    "        y='score',\n",
    "        hue='model',\n",
    "        palette=['skyblue', 'lightgreen'],\n",
    "        capsize=0.1\n",
    "    )\n",
    "    \n",
    "    plt.title('Comparaison des performances des modèles', pad=20)\n",
    "    plt.xlabel('Métrique')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Boxplot des distributions des scores\n",
    "def plot_score_distributions(rf_results, svc_results, metrics):\n",
    "    # Préparation des données pour le boxplot\n",
    "    plot_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Random Forest\n",
    "        for score in rf_results[f'mean_test_{metric}']:\n",
    "            plot_data.append({\n",
    "                'model': 'Random Forest',\n",
    "                'metric': metric,\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "        # SVC\n",
    "        for score in svc_results[f'mean_test_{metric}']:\n",
    "            plot_data.append({\n",
    "                'model': 'SVC',\n",
    "                'metric': metric,\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Création du boxplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(\n",
    "        data=plot_df,\n",
    "        x='metric',\n",
    "        y='score',\n",
    "        hue='model',\n",
    "        palette=['skyblue', 'lightgreen']\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution des scores par métrique et modèle', pad=20)\n",
    "    plt.xlabel('Métrique')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Utilisation des fonctions\n",
    "# Créer le DataFrame de comparaison\n",
    "comparison_df = prepare_comparison_data(rf_results, svc_results, metrics)\n",
    "\n",
    "# Générer les visualisations\n",
    "plot_metrics_comparison(comparison_df)\n",
    "plot_score_distributions(rf_results, svc_results, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    mean_score = rf_results[f'mean_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = rf_results[f'std_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"RandomForest - {metric}: {mean_score:.3f} (+/- {std_score:.3f})\")\n",
    "    \n",
    "    mean_score = svc_results[f'mean_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = svc_results[f'std_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"SVC - {metric}: {mean_score:.3f} (+/- {std_score:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
