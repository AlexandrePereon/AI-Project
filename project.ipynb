{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire de Données (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ydata_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, Normalizer\n",
    "\n",
    "extract_path = './AI_Project_Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_employee_survey_data = os.path.join(extract_path, 'employee_survey_data.csv')\n",
    "csv_manager_survey_data = os.path.join(extract_path, 'manager_survey_data.csv')\n",
    "csv_general_data = os.path.join(extract_path, 'general_data.csv')\n",
    "csv_out_time = os.path.join(extract_path, 'out_time.csv')\n",
    "csv_in_time = os.path.join(extract_path, 'in_time.csv')\n",
    "\n",
    "employee_survey_df = pd.read_csv(csv_employee_survey_data)\n",
    "manager_survey_df = pd.read_csv(csv_manager_survey_data)\n",
    "general_df = pd.read_csv(csv_general_data)\n",
    "out_time_df = pd.read_csv(csv_out_time)\n",
    "in_time_df = pd.read_csv(csv_in_time)\n",
    "\n",
    "# init empty dataframe\n",
    "work_info = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_info.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_columns(df):\n",
    "    return df.loc[:, df.nunique() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical_na(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df.loc[:, categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric_na(df):\n",
    "    for col in df.select_dtypes(include=['number', 'float64']).columns:\n",
    "        median_value = df[col].median()\n",
    "        df.loc[:, col] = df[col].fillna(round(median_value))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_total_working_years(df):\n",
    "    if 'TotalWorkingYears' in df.columns and 'YearsAtCompany' in df.columns:\n",
    "        df.loc[:, 'TotalWorkingYears'] = df['TotalWorkingYears'].fillna(df['YearsAtCompany'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type des valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification des types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_numeric_columns(df):\n",
    "    df = df.apply(lambda col: col.astype(int) if col.dtype == 'float64' and col.dropna().mod(1).eq(0).all() else col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des types `object` en valeurs numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_attrition_to_bool(df):\n",
    "    if 'Attrition' in df.columns:\n",
    "        df.loc[:, 'Attrition'] = df['Attrition'].map({'Yes': True, 'No': False})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_columns(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    if 'Attrition' in categorical_cols:\n",
    "        categorical_cols = categorical_cols.drop('Attrition')\n",
    "\n",
    "    encoder = OneHotEncoder(dtype=int, sparse_output=False)\n",
    "    encoded_array = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)\n",
    "    \n",
    "    df = df.drop(columns=categorical_cols).join(encoded_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des fichiers contenant les horaires d'entrée et sortie des salariés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_and_merge(df):\n",
    "    in_time_melted = in_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='arrival_time')\n",
    "    out_time_melted = out_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='departure_time')\n",
    "\n",
    "    # Renommer la colonne EmployeeID\n",
    "    in_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "    out_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur 'id' et 'date'\n",
    "    merged_clock_in = pd.merge(in_time_melted, out_time_melted, on=['EmployeeID', 'date'], how='outer')\n",
    "    return merged_clock_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_work_column(df):\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'])\n",
    "\n",
    "    # Calculer le temps travaillé (différence entre départ et arrivée)\n",
    "    df['worked_time'] = df['departure_time'] - df['arrival_time']\n",
    "\n",
    "    # Convertir en heures pour avoir un format lisible\n",
    "    df['worked_hours'] = df['worked_time'].dt.total_seconds() / 3600\n",
    "\n",
    "    # Trier par id et date\n",
    "    df.sort_values(by=['EmployeeID', 'date'], inplace=True)\n",
    "\n",
    "    # Moyenne de la durée de travail par jour pour chaque employé\n",
    "    mean_worked_hours = df.groupby('EmployeeID')['worked_hours'].mean()\n",
    "\n",
    "    # Nombre total d'heures travaillées par employé\n",
    "    total_worked_hours = df.groupby('EmployeeID')['worked_hours'].sum()\n",
    "\n",
    "    # Nombre de jours ou le worked_hours est non nul\n",
    "    worked_days = df[df['worked_hours'] > 0].groupby('EmployeeID')['worked_hours'].count()\n",
    "\n",
    "    # Faire un data frame avec ces 3 informations pour chaque employé avec l'EmployeeID comme index\n",
    "    work_info = pd.concat([mean_worked_hours, total_worked_hours, worked_days], axis=1)\n",
    "    work_info.columns = ['mean_worked_hours', 'total_worked_hours', 'worked_days']\n",
    "    return work_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = work_info.select_dtypes(include=[np.number])\n",
    "\n",
    "default_preprocessor = Pipeline([\n",
    "    ('remove_constants', FunctionTransformer(remove_constant_columns, validate=False)),\n",
    "    ('fill_categorical_na', FunctionTransformer(fill_categorical_na, validate=False)),\n",
    "    ('fill_numeric_na', FunctionTransformer(fill_numeric_na, validate=False)),\n",
    "    ('encode_categorical_columns', FunctionTransformer(encode_categorical_columns, validate=False)),\n",
    "    ('simplify_numeric_columns', FunctionTransformer(simplify_numeric_columns, validate=False)),\n",
    "])\n",
    "\n",
    "employee_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "])\n",
    "\n",
    "manager_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "])\n",
    "\n",
    "general_preprocessor = Pipeline([\n",
    "    ('fill_total_working_years', FunctionTransformer(fill_total_working_years, validate=False)),\n",
    "    ('transform_attrition_to_bool', FunctionTransformer(transform_attrition_to_bool, validate=False)),\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "])\n",
    "\n",
    "work_info_preprocessor = Pipeline([\n",
    "    ('reverse_and_merge', FunctionTransformer(reverse_and_merge, validate=False)),\n",
    "    ('generate_work_column', FunctionTransformer(generate_work_column, validate=False)),\n",
    "])\n",
    "\n",
    "# Nettoyage des données\n",
    "employee_survey_data = employee_survey_preprocessor.fit_transform(employee_survey_df)\n",
    "manager_survey_data = manager_survey_preprocessor.fit_transform(manager_survey_df)\n",
    "general_data = general_preprocessor.fit_transform(general_df)\n",
    "work_info = work_info_preprocessor.fit_transform(work_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = general_data.merge(employee_survey_data, on='EmployeeID').merge(manager_survey_data, on='EmployeeID').merge(work_info, on='EmployeeID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YDataProfiling : résumé des différents graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data_report = ydata_profiling.ProfileReport(full_data, title='Full Data')\n",
    "# full_data_report.to_notebook_iframe()\n",
    "# TODO : enlever le commentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportation du dataset modifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(os.path.join(extract_path, 'cleaned_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des caractéristiques ayant le plus d'impact sur le taux d'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, chi2, SelectFromModel, RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_feature_selection_methods(X, y, k_features):\n",
    "    # Dictionnaire pour stocker les features sélectionnées par chaque méthode\n",
    "    selected_features = defaultdict(list)\n",
    "    \n",
    "    # VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.5)\n",
    "    selector.fit_transform(X)\n",
    "    selected_features['VarianceThreshold'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # SelectKBest\n",
    "    selector = SelectKBest(chi2, k=k_features)\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['SelectKBest'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # SelectFromModel\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['SelectFromModel'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # RFE\n",
    "    selector = RFE(RandomForestClassifier(n_estimators=100), \n",
    "                  n_features_to_select=k_features, \n",
    "                  step=1)\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['RFE'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # RFECV\n",
    "    selector = RFECV(RandomForestClassifier(n_estimators=100), \n",
    "                    min_features_to_select=k_features,\n",
    "                    step=1, \n",
    "                    cv=5)\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['RFECV'] = list(X.columns[selector.support_])\n",
    "    \n",
    "    # Créer un DataFrame avec le nombre maximal de caractéristiques\n",
    "    max_features = max(len(features) for features in selected_features.values())\n",
    "    \n",
    "    # Remplir avec None pour avoir des colonnes de même longueur\n",
    "    for method in selected_features:\n",
    "        selected_features[method].extend([None] * (max_features - len(selected_features[method])))\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    features_table = pd.DataFrame(selected_features)\n",
    "    \n",
    "    # Ajouter des statistiques sur les features sélectionnées\n",
    "    print(\"\\nNombre de caractéristiques sélectionnées par méthode:\")\n",
    "    for method in selected_features:\n",
    "        count = sum(1 for x in selected_features[method] if x is not None)\n",
    "        print(f\"{method}: {count} features\")\n",
    "    \n",
    "    return features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation\n",
    "X = full_data.drop(columns=['EmployeeID', 'Attrition'])\n",
    "y = full_data['Attrition'].astype(int)\n",
    "\n",
    "features_table = compare_feature_selection_methods(X, y, k_features=15)\n",
    "print(\"\\nTableau des caractéristiques sélectionnées:\")\n",
    "features_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélectionner le modèle le plus performant pour prédire l'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Load the data\n",
    "filename = './AI_Project_Data/cleaned_data.csv'\n",
    "employee_data = pd.read_csv(filename)\n",
    "\n",
    "# Displaying data.head() to see the first 5 rows of the data\n",
    "employee_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des valeurs d'attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation du dataset en plusieurs échantillons pour la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "employee_df = employee_data.copy()\n",
    "\n",
    "y = employee_df[\"Attrition\"]\n",
    "X = employee_df.drop(columns=[\"Attrition\"])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42), \n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des dictionnaires pour stocker les scores\n",
    "scores = []\n",
    "predictions = {}\n",
    "metrics = {\n",
    "    'precision': {name: [] for name in models.keys()},\n",
    "    'recall': {name: [] for name in models.keys()},\n",
    "    'f1': {name: [] for name in models.keys()},\n",
    "    'auc': {name: [] for name in models.keys()},\n",
    "    'accuracy': {name: [] for name in models.keys()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Prédictions\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions[name] = model.predict(X_test)    \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Stockage des métriques\n",
    "        metrics['precision'][name].append(precision)\n",
    "        metrics['recall'][name].append(recall)\n",
    "        metrics['f1'][name].append(f1)\n",
    "        metrics['auc'][name].append(auc)\n",
    "        metrics['accuracy'][name].append(accuracy)\n",
    "        # Ajout des scores dans le DataFrame final\n",
    "        scores.append({\n",
    "            'Model': name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Accuracy': accuracy,\n",
    "            'AUC': auc\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df_mean = scores_df.groupby('Model').mean()\n",
    "scores_df_std = scores_df.groupby('Model').std()\n",
    "scores_df_mean = scores_df_mean.sort_values(['F1 Score', 'Precision', 'Accuracy', 'Recall'], ascending=False) # TODO : change order\n",
    "scores_df_std = scores_df_std.sort_values(['F1 Score', 'Precision', 'Accuracy', 'Recall'], ascending=True) # TODO : change order\n",
    "\n",
    "print('Mean Scores')\n",
    "print(scores_df_mean)\n",
    "print('_'*50)\n",
    "print('Standard Deviation')\n",
    "print(scores_df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_params_grid = {\n",
    "    # Most important parameters\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    \n",
    "    # Basic settings\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'random_state': [42],\n",
    "    'probability': [True]\n",
    "}\n",
    "\n",
    "# creating model using GridSearchCV\n",
    "SVC_model = GridSearchCV(\n",
    "    estimator=SVC(probability=True),\n",
    "    param_grid=svc_params_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',\n",
    "    verbose=0,\n",
    "    error_score='raise',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Définition d'une grille simplifiée de paramètres\n",
    "rf_params_grid = {\n",
    "    # Paramètres essentiels\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    \n",
    "    # Paramètres de base\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Configuration du GridSearchCV avec des options avancées\n",
    "RandomForest_model = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=rf_params_grid,\n",
    "    cv=5,  # Validation croisée à 5 plis\n",
    "    n_jobs=-1,  # Utilise tous les cœurs disponibles\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',  # Réentraîne sur la meilleure métrique accuracy\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# model fitting\n",
    "RandomForest_model.fit(X_train, y_train)\n",
    "SVC_model.fit(X_train, y_train)\n",
    "\n",
    "# getting best parameters\n",
    "best_rf_model = RandomForest_model.best_estimator_\n",
    "best_svc_model = SVC_model.best_estimator_\n",
    "\n",
    "# displaying results\n",
    "print(\"\\nMeilleurs paramètres trouvés :\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_params_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_params_}\")\n",
    "print(\"\\nMeilleur score de validation croisée:\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_score_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_score_}\")\n",
    "\n",
    "\n",
    "# displaying scores for metrics\n",
    "rf_results = pd.DataFrame(RandomForest_model.cv_results_)\n",
    "svc_results = pd.DataFrame(SVC_model.cv_results_)\n",
    "print(\"\\nRésultats détaillés pour la meilleure configuration:\")\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "for metric in metrics:\n",
    "    mean_score = rf_results[f'mean_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = rf_results[f'std_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"RandomForest - {metric}: {mean_score:.3f} (+/- {std_score*2:.3f})\")\n",
    "    \n",
    "    mean_score = svc_results[f'mean_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = svc_results[f'std_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"SVC - {metric}: {mean_score:.3f} (+/- {std_score*2:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
