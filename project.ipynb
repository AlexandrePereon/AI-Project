{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire de Données (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ydata_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "\n",
    "extract_path = './AI_Project_Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_employee_survey_data = os.path.join(extract_path, 'employee_survey_data.csv')\n",
    "csv_manager_survey_data = os.path.join(extract_path, 'manager_survey_data.csv')\n",
    "csv_general_data = os.path.join(extract_path, 'general_data.csv')\n",
    "csv_out_time = os.path.join(extract_path, 'out_time.csv')\n",
    "csv_in_time = os.path.join(extract_path, 'in_time.csv')\n",
    "\n",
    "employee_survey_df = pd.read_csv(csv_employee_survey_data)\n",
    "manager_survey_df = pd.read_csv(csv_manager_survey_data)\n",
    "general_df = pd.read_csv(csv_general_data)\n",
    "out_time_df = pd.read_csv(csv_out_time)\n",
    "in_time_df = pd.read_csv(csv_in_time)\n",
    "\n",
    "# init empty dataframe\n",
    "work_info = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_info.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constant_columns(df):\n",
    "    return df.loc[:, df.nunique() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical_na(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df.loc[:, categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric_na(df):\n",
    "    for col in df.select_dtypes(include=['number', 'float64']).columns:\n",
    "        median_value = df[col].median()\n",
    "        df.loc[:, col] = df[col].fillna(round(median_value))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_total_working_years(df):\n",
    "    if 'TotalWorkingYears' in df.columns and 'YearsAtCompany' in df.columns:\n",
    "        df.loc[:, 'TotalWorkingYears'] = df['TotalWorkingYears'].fillna(df['YearsAtCompany'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type des valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification des types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_numeric_columns(df):\n",
    "    df = df.apply(lambda col: col.astype(int) if col.dtype == 'float64' and col.dropna().mod(1).eq(0).all() else col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des types `object` en valeurs numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_attrition_to_bool(df):\n",
    "    if 'Attrition' in df.columns:\n",
    "        df.loc[:, 'Attrition'] = df['Attrition'].map({'Yes': True, 'No': False})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestion des fichiers contenant les horaires d'entrée et sortie des salariés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_and_merge(df):\n",
    "    in_time_melted = in_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='arrival_time')\n",
    "    out_time_melted = out_time_df.melt(id_vars=['Unnamed: 0'], var_name='date', value_name='departure_time')\n",
    "\n",
    "    # Renommer la colonne EmployeeID\n",
    "    in_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "    out_time_melted.rename(columns={'Unnamed: 0': 'EmployeeID'}, inplace=True)\n",
    "\n",
    "    # Fusionner les deux DataFrames sur 'id' et 'date'\n",
    "    merged_clock_in = pd.merge(in_time_melted, out_time_melted, on=['EmployeeID', 'date'], how='outer')\n",
    "    return merged_clock_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_work_column(df):\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'])\n",
    "\n",
    "    # Calculer le temps travaillé (différence entre départ et arrivée)\n",
    "    df['worked_time'] = df['departure_time'] - df['arrival_time']\n",
    "\n",
    "    # Convertir en heures pour avoir un format lisible\n",
    "    df['worked_hours'] = df['worked_time'].dt.total_seconds() / 3600\n",
    "\n",
    "    # Trier par id et date\n",
    "    df.sort_values(by=['EmployeeID', 'date'], inplace=True)\n",
    "\n",
    "    # Moyenne de la durée de travail par jour pour chaque employé\n",
    "    mean_worked_hours = df.groupby('EmployeeID')['worked_hours'].mean()\n",
    "\n",
    "    # Nombre total d'heures travaillées par employé\n",
    "    total_worked_hours = df.groupby('EmployeeID')['worked_hours'].sum()\n",
    "\n",
    "    # Nombre de jours ou le worked_hours est non nul\n",
    "    worked_days = df[df['worked_hours'] > 0].groupby('EmployeeID')['worked_hours'].count()\n",
    "\n",
    "    # Faire un data frame avec ces 3 informations pour chaque employé avec l'EmployeeID comme index\n",
    "    work_info = pd.concat([mean_worked_hours, total_worked_hours, worked_days], axis=1)\n",
    "    work_info.columns = ['mean_worked_hours', 'total_worked_hours', 'worked_days']\n",
    "    return work_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employee_survey_to_cat(df):\n",
    "    df['EnvironmentSatisfaction'] = df['EnvironmentSatisfaction'].replace({1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Excellent'})\n",
    "    df['JobSatisfaction'] = df['JobSatisfaction'].replace({1: 'Dissatisfied', 2: 'Neutral', 3: 'Satisfied', 4: 'Very Satisfied'})\n",
    "    df['WorkLifeBalance'] = df['WorkLifeBalance'].replace({1: 'Poor', 2: 'Average', 3: 'Good', 4: 'Excellent'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manager_survey_to_cat(df):\n",
    "    df['JobInvolvement'] = df['JobInvolvement'].replace({1: 'Not Engaged', 2: 'Moderately Engaged', 3: 'Highly Engaged', 4: 'Fully Committed'})\n",
    "    df['PerformanceRating'] = df['PerformanceRating'].replace({1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Outstanding'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_to_cat(df):\n",
    "    df['Education'] = df['Education'].replace({\n",
    "        1: 'HighSchool', \n",
    "        2: 'Associate', \n",
    "        3: 'Bachelor', \n",
    "        4: 'Master', \n",
    "        5: 'PhD'\n",
    "    })\n",
    "    df['StockOptionLevel'] = df['StockOptionLevel'].replace({  \n",
    "        0: 'None',  \n",
    "        1: 'Low',  \n",
    "        2: 'Medium',  \n",
    "        3: 'High'  \n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_preprocessor = Pipeline([\n",
    "    ('remove_constants', FunctionTransformer(remove_constant_columns, validate=False)),\n",
    "    ('fill_categorical_na', FunctionTransformer(fill_categorical_na, validate=False)),\n",
    "    ('fill_numeric_na', FunctionTransformer(fill_numeric_na, validate=False)),\n",
    "    ('simplify_numeric_columns', FunctionTransformer(simplify_numeric_columns, validate=False)),\n",
    "])\n",
    "\n",
    "employee_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "    ('employee_survey_to_cat', FunctionTransformer(employee_survey_to_cat, validate=False)),\n",
    "])\n",
    "\n",
    "manager_survey_preprocessor = Pipeline([\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "    ('manager_survey_to_cat', FunctionTransformer(manager_survey_to_cat, validate=False)),\n",
    "])\n",
    "\n",
    "general_preprocessor = Pipeline([\n",
    "    ('fill_total_working_years', FunctionTransformer(fill_total_working_years, validate=False)),\n",
    "    ('transform_attrition_to_bool', FunctionTransformer(transform_attrition_to_bool, validate=False)),\n",
    "    ('general_to_cat', FunctionTransformer(general_to_cat, validate=False)),\n",
    "    ('default_preprocessor', default_preprocessor),\n",
    "])\n",
    "\n",
    "work_info_preprocessor = Pipeline([\n",
    "    ('reverse_and_merge', FunctionTransformer(reverse_and_merge, validate=False)),\n",
    "    ('generate_work_column', FunctionTransformer(generate_work_column, validate=False)),\n",
    "])\n",
    "\n",
    "# Nettoyage des données\n",
    "employee_survey_data = employee_survey_preprocessor.fit_transform(employee_survey_df)\n",
    "manager_survey_data = manager_survey_preprocessor.fit_transform(manager_survey_df)\n",
    "general_data = general_preprocessor.fit_transform(general_df)\n",
    "work_info = work_info_preprocessor.fit_transform(work_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = general_data.merge(employee_survey_data, on='EmployeeID').merge(manager_survey_data, on='EmployeeID').merge(work_info, on='EmployeeID')\n",
    "\n",
    "clean_data = clean_data.drop(columns=['EmployeeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_num = clean_data.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(clean_data[clean_data_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(clean_data[clean_data_num].corr(), cmap='coolwarm')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = clean_data.select_dtypes(include=[object]).drop(columns=['Attrition'])\n",
    "\n",
    "clean_data_cat = pd.get_dummies(cat_columns, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "clean_data_num = pd.DataFrame(scaler.fit_transform(clean_data[clean_data_num]), columns=clean_data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = pd.concat([clean_data_num, clean_data_cat, clean_data['Attrition']], axis=1)\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = concat_data[concat_data['Attrition'] == True]\n",
    "df_false = concat_data[concat_data['Attrition'] == False]\n",
    "\n",
    "\n",
    "min_count = min(len(df_true), len(df_false))\n",
    "\n",
    "df_true_sampled = df_true.sample(n=min_count, random_state=42)\n",
    "df_false_sampled = df_false.sample(n=min_count, random_state=42)\n",
    "\n",
    "\n",
    "full_data = pd.concat([df_true_sampled, df_false_sampled])\n",
    "full_data['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YDataProfiling : résumé des différents graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data_report = ydata_profiling.ProfileReport(full_data, title='Full Data')\n",
    "# final_data_report.to_notebook_iframe()\n",
    "# TODO : enlever le commentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportation du dataset modifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(os.path.join(extract_path, 'cleaned_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection des caractéristiques ayant le plus d'impact sur le taux d'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_feature_selection_methods(X, y, k_features):\n",
    "    # Dictionnaire pour stocker les features sélectionnées par chaque méthode\n",
    "    selected_features = defaultdict(list)\n",
    "    \n",
    "    # VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.5)\n",
    "    selector.fit_transform(X)\n",
    "    selected_features['VarianceThreshold'] = list(X.columns[selector.get_support()])\n",
    "\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['SelectFromModel'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # RFE\n",
    "    selector = RFE(RandomForestClassifier(n_estimators=100), \n",
    "                  n_features_to_select=k_features, \n",
    "                  step=1)\n",
    "    selector.fit_transform(X, y)\n",
    "    selected_features['RFE'] = list(X.columns[selector.get_support()])\n",
    "    \n",
    "    # Créer un DataFrame avec le nombre maximal de caractéristiques\n",
    "    max_features = max(len(features) for features in selected_features.values())\n",
    "    \n",
    "    # Remplir avec None pour avoir des colonnes de même longueur\n",
    "    for method in selected_features:\n",
    "        selected_features[method].extend([None] * (max_features - len(selected_features[method])))\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    features_table = pd.DataFrame(selected_features)\n",
    "    \n",
    "    # Ajouter des statistiques sur les features sélectionnées\n",
    "    print(\"\\nNombre de caractéristiques sélectionnées par méthode:\")\n",
    "    for method in selected_features:\n",
    "        count = sum(1 for x in selected_features[method] if x is not None)\n",
    "        print(f\"{method}: {count} features\")\n",
    "    \n",
    "    return features_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation\n",
    "X = full_data.drop(columns=['Attrition'])\n",
    "y = full_data['Attrition'].astype(int)\n",
    "\n",
    "features_table = compare_feature_selection_methods(X, y, k_features=15)\n",
    "print(\"\\nTableau des caractéristiques sélectionnées:\")\n",
    "features_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélectionner le modèle le plus performant pour prédire l'attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Load the data\n",
    "filename = './AI_Project_Data/cleaned_data.csv'\n",
    "employee_data = pd.read_csv(filename)\n",
    "\n",
    "# Displaying data.head() to see the first 5 rows of the data\n",
    "employee_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des valeurs d'attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation du dataset en plusieurs échantillons pour la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "employee_df = employee_data.copy()\n",
    "\n",
    "y = employee_df[\"Attrition\"]\n",
    "X = employee_df.drop(columns=[\"Attrition\"])\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "    \"SVC\": SVC(probability=True, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42), \n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des dictionnaires pour stocker les scores\n",
    "scores = []\n",
    "predictions = {}\n",
    "metrics = {\n",
    "    'precision': {name: [] for name in models.keys()},\n",
    "    'recall': {name: [] for name in models.keys()},\n",
    "    'f1': {name: [] for name in models.keys()},\n",
    "    'auc': {name: [] for name in models.keys()},\n",
    "    'accuracy': {name: [] for name in models.keys()}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Prédictions\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions[name] = model.predict(X_test)    \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Stockage des métriques\n",
    "        metrics['precision'][name].append(precision)\n",
    "        metrics['recall'][name].append(recall)\n",
    "        metrics['f1'][name].append(f1)\n",
    "        metrics['auc'][name].append(auc)\n",
    "        metrics['accuracy'][name].append(accuracy)\n",
    "        # Ajout des scores dans le DataFrame final\n",
    "        scores.append({\n",
    "            'Model': name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Accuracy': accuracy,\n",
    "            'AUC': auc\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df_mean = scores_df.groupby('Model').mean()\n",
    "scores_df_std = scores_df.groupby('Model').std()\n",
    "scores_df_mean = scores_df_mean.sort_values(['F1 Score', 'Precision', 'Recall', 'Accuracy'], ascending=False) # TODO : change order\n",
    "scores_df_std = scores_df_std.sort_values(['F1 Score', 'Precision', 'Recall', 'Accuracy'], ascending=True) # TODO : change order\n",
    "\n",
    "print('Mean Scores')\n",
    "print(scores_df_mean)\n",
    "print('_'*50)\n",
    "print('Standard Deviation')\n",
    "print(scores_df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des temps d'entrainement et de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "training_times = []\n",
    "prediction_times = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    training_times.append(end_time - start_time)\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    prediction_times.append(end_time - start_time)\n",
    "\n",
    "# Create DataFrame to store times\n",
    "time_df = pd.DataFrame({\n",
    "    'Model': models.keys(),\n",
    "    'Training Time (s)': training_times,\n",
    "    'Prediction Time (s)': prediction_times\n",
    "})\n",
    "\n",
    "time_df = time_df.sort_values(['Prediction Time (s)', 'Training Time (s)'], ascending=True) # TODO : change order\n",
    "print(time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'importance des différentes variables du dataset sur la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def get_feature_importance(model, X, y, model_name):\n",
    "    feature_names = X.columns\n",
    "    importance_dict = {}\n",
    "    \n",
    "    if model_name == \"RandomForest\" or model_name == \"DecisionTreeClassifier\":\n",
    "        # Ces modèles ont feature_importances_ natif\n",
    "        importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "        \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        # Pour la régression logistique, on utilise les coefficients\n",
    "        if len(model.classes_) == 2:  # Classification binaire\n",
    "            importance_dict = dict(zip(feature_names, np.abs(model.coef_[0])))\n",
    "        else:  # Classification multi-classe\n",
    "            # Moyenne des valeurs absolues des coefficients pour chaque classe\n",
    "            importance_dict = dict(zip(feature_names, np.mean(np.abs(model.coef_), axis=0)))\n",
    "            \n",
    "    elif model_name == \"SVC\":\n",
    "        # Pour SVC, on utilise permutation_importance car pas d'importance native\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "        importance_dict = dict(zip(feature_names, result.importances_mean))\n",
    "    \n",
    "    return importance_dict\n",
    "\n",
    "def plot_feature_importance(importance_dict, model_name, top_n=50):\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Conversion en DataFrame pour faciliter la manipulation\n",
    "    df_importance = pd.DataFrame({\n",
    "        'feature': importance_dict.keys(),\n",
    "        'importance': importance_dict.values()\n",
    "    })\n",
    "    \n",
    "    # Tri par importance décroissante et sélection des top_n features\n",
    "    df_importance = df_importance.sort_values('importance', ascending=True).tail(top_n)\n",
    "    \n",
    "    # Création du graphique\n",
    "    sns.barplot(data=df_importance, x='importance', y='feature', palette=\"viridis\")\n",
    "    \n",
    "    plt.title(f'Top {top_n} Features Importance - {model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for i, v in enumerate(df_importance['importance']):\n",
    "        plt.text(v, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name} Feature Importance:\")\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Calcul et affichage des importances\n",
    "    importance_dict = get_feature_importance(model, X, y, model_name)\n",
    "    \n",
    "    # Visualisation\n",
    "    plot_feature_importance(importance_dict, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_params_grid = {\n",
    "    # Most important parameters\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto', 0.01],\n",
    "    \n",
    "    # Basic settings\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'random_state': [42],\n",
    "    'probability': [True]\n",
    "}\n",
    "\n",
    "# creating model using GridSearchCV\n",
    "SVC_model = GridSearchCV(\n",
    "    estimator=SVC(probability=True),\n",
    "    param_grid=svc_params_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',\n",
    "    verbose=0,\n",
    "    error_score='raise',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Définition d'une grille simplifiée de paramètres\n",
    "rf_params_grid = {\n",
    "    # Paramètres essentiels\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    \n",
    "    # Paramètres de base\n",
    "    'random_state': [42],\n",
    "    'n_jobs': [-1],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Configuration du GridSearchCV avec des options avancées\n",
    "RandomForest_model = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=rf_params_grid,\n",
    "    cv=5,  # Validation croisée à 5 plis\n",
    "    n_jobs=-1,  # Utilise tous les cœurs disponibles\n",
    "    scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],\n",
    "    refit='accuracy',  # Réentraîne sur la meilleure métrique accuracy\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# model fitting\n",
    "RandomForest_model.fit(X_train, y_train)\n",
    "SVC_model.fit(X_train, y_train)\n",
    "\n",
    "# getting best parameters\n",
    "best_rf_model = RandomForest_model.best_estimator_\n",
    "best_svc_model = SVC_model.best_estimator_\n",
    "\n",
    "# displaying results\n",
    "print(\"\\nMeilleurs paramètres trouvés :\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_params_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_params_}\")\n",
    "print(\"\\nMeilleur score de validation croisée:\")\n",
    "print(f\"\\nRandomForest {RandomForest_model.best_score_}\")\n",
    "print(f\"\\nSVC {SVC_model.best_score_}\")\n",
    "\n",
    "\n",
    "# displaying scores for metrics\n",
    "rf_results = pd.DataFrame(RandomForest_model.cv_results_)\n",
    "svc_results = pd.DataFrame(SVC_model.cv_results_)\n",
    "print(\"\\nRésultats détaillés pour la meilleure configuration:\")\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour la visualisation\n",
    "def prepare_comparison_data(rf_results, svc_results, metrics):\n",
    "    # Création d'un dataframe pour la comparaison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Random Forest\n",
    "        rf_scores = {\n",
    "            'model': 'Random Forest',\n",
    "            'metric': metric,\n",
    "            'score': rf_results[f'mean_test_{metric}'].mean(),\n",
    "            'std': rf_results[f'std_test_{metric}'].mean()\n",
    "        }\n",
    "        comparison_data.append(rf_scores)\n",
    "        \n",
    "        # SVC\n",
    "        svc_scores = {\n",
    "            'model': 'SVC',\n",
    "            'metric': metric,\n",
    "            'score': svc_results[f'mean_test_{metric}'].mean(),\n",
    "            'std': svc_results[f'std_test_{metric}'].mean()\n",
    "        }\n",
    "        comparison_data.append(svc_scores)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# 1. Graphique comparatif des métriques\n",
    "def plot_metrics_comparison(comparison_data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Création du barplot\n",
    "    ax = sns.barplot(\n",
    "        data=comparison_data,\n",
    "        x='metric',\n",
    "        y='score',\n",
    "        hue='model',\n",
    "        palette=['skyblue', 'lightgreen'],\n",
    "        capsize=0.1\n",
    "    )\n",
    "    \n",
    "    plt.title('Comparaison des performances des modèles', pad=20)\n",
    "    plt.xlabel('Métrique')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Boxplot des distributions des scores\n",
    "def plot_score_distributions(rf_results, svc_results, metrics):\n",
    "    # Préparation des données pour le boxplot\n",
    "    plot_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Random Forest\n",
    "        for score in rf_results[f'mean_test_{metric}']:\n",
    "            plot_data.append({\n",
    "                'model': 'Random Forest',\n",
    "                'metric': metric,\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "        # SVC\n",
    "        for score in svc_results[f'mean_test_{metric}']:\n",
    "            plot_data.append({\n",
    "                'model': 'SVC',\n",
    "                'metric': metric,\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Création du boxplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(\n",
    "        data=plot_df,\n",
    "        x='metric',\n",
    "        y='score',\n",
    "        hue='model',\n",
    "        palette=['skyblue', 'lightgreen']\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution des scores par métrique et modèle', pad=20)\n",
    "    plt.xlabel('Métrique')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Utilisation des fonctions\n",
    "# Créer le DataFrame de comparaison\n",
    "comparison_df = prepare_comparison_data(rf_results, svc_results, metrics)\n",
    "\n",
    "# Générer les visualisations\n",
    "plot_metrics_comparison(comparison_df)\n",
    "plot_score_distributions(rf_results, svc_results, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    mean_score = rf_results[f'mean_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = rf_results[f'std_test_{metric}'].iloc[rf_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"RandomForest - {metric}: {mean_score:.3f} (+/- {std_score:.3f})\")\n",
    "    \n",
    "    mean_score = svc_results[f'mean_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    std_score = svc_results[f'std_test_{metric}'].iloc[svc_results['rank_test_accuracy'].argmin()]\n",
    "    print(f\"SVC - {metric}: {mean_score:.3f} (+/- {std_score:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
